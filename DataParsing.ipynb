{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a11f1d4",
   "metadata": {},
   "source": [
    "# Robust and Efficient DeepFake Detection via Quantization and Adversarial Training \n",
    "## By Rahul Champaneria\n",
    "\n",
    "**Dataset:** _OpenForensics_\n",
    "\n",
    "**Model Type**: _RECCE_\n",
    "\n",
    "**Quantization Types:**\n",
    "* Dynamic\n",
    "    * _Baseline INT8-Dynamic_ ```recce_standard_int8_dynamic.pt```\n",
    "    * _PGD-AT INT8-Dynamic_ ```recce_int8dyn_pgd_at.pt```\n",
    "    * _TRADES INT8-Dynamic_ ```recce_int8dyn_trades.pt```\n",
    "* Static\n",
    "    * _Baseline INT8-Static-FX_ ```recce_standard_int8_static_fx.pt```\n",
    "    * _Standard INT8-Static-FX_ ```recce_int8static_fx_standard.pt```\n",
    "* Quantized Aware Training (QAT)\n",
    "    * _Baseline INT8-QAT-FX_ ```recce_standard_int8_qat_fx.pt```\n",
    "    * _Standard INT8-QAT-FX_ ```recce_int8qat_fx_standard.pt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aa5b194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torchvision) (2.3.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: timm in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (1.0.22)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from timm) (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from timm) (0.24.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from timm) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from timm) (1.1.4)\n",
      "Requirement already satisfied: safetensors in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from huggingface_hub->timm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from huggingface_hub->timm) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from huggingface_hub->timm) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from huggingface_hub->timm) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from huggingface_hub->timm) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub->timm) (1.3.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch->timm) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch->timm) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jinja2->torch->timm) (3.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torchvision->timm) (2.3.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torchvision->timm) (12.0.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from typer-slim->huggingface_hub->timm) (8.3.1)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (1.8.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torchmetrics) (2.3.4)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torchmetrics) (25.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torchmetrics) (2.9.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torchmetrics) (0.15.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (80.9.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from matplotlib) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tensorboard) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tensorboard) (3.10)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tensorboard) (2.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tensorboard) (12.0.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tensorboard) (6.33.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tensorboard) (80.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (6.0.3)\n",
      "Requirement already satisfied: rich in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (14.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from rich) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from rich) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: jupyter in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (8.1.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: notebook in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter) (7.4.7)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter) (7.1.0)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter) (4.4.10)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipywidgets) (9.7.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipykernel->jupyter) (1.8.17)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipykernel->jupyter) (5.9.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipykernel->jupyter) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipykernel->jupyter) (7.1.3)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipykernel->jupyter) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from ipykernel->jupyter) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->jupyter) (1.17.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab->jupyter) (2.0.5)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab->jupyter) (2.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab->jupyter) (2.17.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab->jupyter) (2.28.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab->jupyter) (80.9.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (5.10.4)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.23.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.9.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.12.1)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.25.1)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter) (4.15.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jinja2>=3.0.3->jupyterlab->jupyter) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.29.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (4.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.3)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.1.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.10.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from nbconvert->jupyter) (4.14.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.3.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from nbconvert->jupyter) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from nbconvert->jupyter) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.21.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.5.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.23)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.8)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.4.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\admin\\github\\dfdetectadversattack\\.venv\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2025.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'conda' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --upgrade\n",
    "\n",
    "!pip install timm\n",
    "\n",
    "!pip install torchmetrics\n",
    "\n",
    "!pip install numpy pandas scikit-learn\n",
    "\n",
    "!pip install matplotlib seaborn tqdm\n",
    "\n",
    "!pip install tensorboard\n",
    "\n",
    "!pip install pyyaml rich\n",
    "\n",
    "!pip install --upgrade jupyter ipywidgets tqdm\n",
    "\n",
    "!conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c042b",
   "metadata": {},
   "source": [
    "## Part 1 - Environment Setup & Dataset Overview\n",
    "Loading of Dataset location, Pytorch, TorchVision, TorchMetrics, and other necessary libraries for:\n",
    "* Model Construction\n",
    "* Dataset Preprocessing\n",
    "* Training Loops\n",
    "* Adversarial Attacks\n",
    "* Quantization\n",
    "\n",
    "```text\n",
    "Dataset/\n",
    "├── Train/\n",
    "│   ├── real/\n",
    "│   └── fake/\n",
    "├── Validation/\n",
    "│   ├── real/\n",
    "│   └── fake/\n",
    "└── Test/\n",
    "    ├── real/\n",
    "    └── fake/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671aaa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRAIN]\n",
      " Real Images: 70001\n",
      " Fake Images: 70001\n",
      "\n",
      "[VALIDATION]\n",
      " Real Images: 19787\n",
      " Fake Images: 19641\n",
      "\n",
      "[TEST]\n",
      " Real Images: 5413\n",
      " Fake Images: 5492\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_ROOT = r\"C:\\Users\\admin\\Github\\DFDetectAdversAttack\\Dataset\"\n",
    "\n",
    "SEED = 42\n",
    "IMSIZE = 224\n",
    "BATCH = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "PGD_EPS = 8/255\n",
    "PGD_ALPHA = 2/255\n",
    "PGD_STEPS = 10\n",
    "TRADES_BETA = 6.0\n",
    "\n",
    "for split in [\"Train\",\"Validation\",\"Test\"]:\n",
    "    real_path = os.path.join(DATA_ROOT, split, \"real\")\n",
    "    fake_path = os.path.join(DATA_ROOT, split, \"fake\")\n",
    "    print(f\"\\n[{split.upper()}]\")\n",
    "    print(\" Real Images:\", len(os.listdir(real_path)))\n",
    "    print(\" Fake Images:\", len(os.listdir(fake_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3676f",
   "metadata": {},
   "source": [
    "## Part 2 - Core Libraries, Device Setup, and Metric Utilities\n",
    "\n",
    "This section:\n",
    "- Imports all core libraries (PyTorch, TorchVision, TorchMetrics, NumPy, etc.).\n",
    "- Sets the `device` (GPU if available, otherwise CPU) and random seeds for reproducibility.\n",
    "- Defines:\n",
    "  - ImageNet normalization (`mean`, `std`)\n",
    "  - `make_metrics()` to create AUROC, AUPRC, and F1 metric objects\n",
    "  - `denorm_bounds()` and `to_norm_eps()` to convert pixel-space epsilon/alpha to normalized space\n",
    "\n",
    "These helpers are reused by training, PGD, TRADES, and robustness evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22db330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, copy, json, math, numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "import timm\n",
    "\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryAveragePrecision, BinaryF1Score\n",
    "\n",
    "# Repro\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# Normalization (ImageNet)\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std  = (0.229, 0.224, 0.225)\n",
    "\n",
    "def make_metrics():\n",
    "    return (BinaryAUROC().to(device),\n",
    "            BinaryAveragePrecision().to(device),\n",
    "            BinaryF1Score().to(device))\n",
    "\n",
    "def denorm_bounds():\n",
    "    lo = (torch.tensor(0.0) - torch.tensor(mean).view(3,1,1)) / torch.tensor(std).view(3,1,1)\n",
    "    hi = (torch.tensor(1.0) - torch.tensor(mean).view(3,1,1)) / torch.tensor(std).view(3,1,1)\n",
    "    return lo.to(device), hi.to(device)\n",
    "\n",
    "def to_norm_eps(eps_pixel=PGD_EPS, alpha_pixel=PGD_ALPHA):\n",
    "    eps_n = torch.tensor(eps_pixel, device=device) / torch.tensor(std, device=device).view(3,1,1)\n",
    "    alpha_n = torch.tensor(alpha_pixel, device=device) / torch.tensor(std, device=device).view(3,1,1)\n",
    "    return eps_n, alpha_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad284a91",
   "metadata": {},
   "source": [
    "## Part 3 — Data Transforms, Datasets, and DataLoaders\n",
    "\n",
    "This section:\n",
    "- Defines data augmentations for training (random crop, flip, color jitter) and deterministic transforms for validation/test.\n",
    "- Builds `ImageFolder` datasets for `Train`, `Validation`, and `Test`.\n",
    "- Wraps them into `DataLoader`s.\n",
    "- Checks that the task is binary (`NUM_CLASSES == 2`) and prints the class names.\n",
    "- Probes one mini-batch to verify shapes and value range after normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88a5383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Fake', 'Real']\n",
      "Probe batch: torch.Size([8, 3, 224, 224]) torch.Size([8]) min/max -2.1179039478302 2.640000104904175\n"
     ]
    }
   ],
   "source": [
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMSIZE, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std)\n",
    "])\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize(IMSIZE+32),\n",
    "    transforms.CenterCrop(IMSIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std)\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(os.path.join(DATA_ROOT, \"Train\"), transform=train_tf)\n",
    "val_ds   = datasets.ImageFolder(os.path.join(DATA_ROOT, \"Validation\"),   transform=eval_tf)\n",
    "test_ds  = datasets.ImageFolder(os.path.join(DATA_ROOT, \"Test\"),  transform=eval_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "classes = train_ds.classes\n",
    "NUM_CLASSES = len(classes)\n",
    "assert NUM_CLASSES == 2, f\"This notebook assumes binary classification; got {classes}\"\n",
    "print(\"Classes:\", classes)\n",
    "# quick loader probe\n",
    "probe_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=8, shuffle=True, num_workers=0, pin_memory=False\n",
    ")\n",
    "xb, yb = next(iter(probe_loader))\n",
    "print(\"Probe batch:\", xb.shape, yb.shape, \"min/max\", float(xb.min()), float(xb.max()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58feedfc",
   "metadata": {},
   "source": [
    "## Part 4 — Model Architectures (SRM, RECCE, SRMNet, and Factory)\n",
    "\n",
    "This section:\n",
    "- Defines `SRMConv2d`, a fixed high-pass filter block over the luminance channel.\n",
    "- Defines `RECCE`:\n",
    "  - Extracts spatial cues (Sobel X/Y, Laplacian) from grayscale.\n",
    "  - Concatenates them with RGB to form a 6-channel input.\n",
    "  - Uses a ResNet-18 backbone with `QuantStub`/`DeQuantStub` to support quantization.\n",
    "- Loads a pretrained FP32 RECCE model from `./runs/recce_standard_fp32_final.pt`.\n",
    "- Defines `SRMNet`, which uses SRM as a frontend and ResNet-18 as the classifier.\n",
    "- Provides `make_xception()` and `make_model(kind)` as a factory to instantiate `xception`, `recce`, or `srm` models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f8d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained FP32 RECCE.\n"
     ]
    }
   ],
   "source": [
    "class SRMConv2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        k = np.stack([\n",
    "            [[0,0,0,0,0],[0,0,0,0,0],[0,-1,2,-1,0],[0,0,0,0,0],[0,0,0,0,0]],\n",
    "            [[0,0,0,0,0],[0,-1,2,-1,0],[0,2,-4,2,0],[0,-1,2,-1,0],[0,0,0,0,0]],\n",
    "            [[-1,2,-1,2,-1],[2,-6,8,-6,2],[-1,8,-12,8,-1],[2,-6,8,-6,2],[-1,2,-1,2,-1]]\n",
    "        ]).astype(np.float32)\n",
    "        k = k[:,None,:,:]  # (3,1,5,5)\n",
    "        self.weight = nn.Parameter(torch.from_numpy(k), requires_grad=False)\n",
    "        self.bias   = nn.Parameter(torch.zeros(3), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):  # x: (N,3,H,W) RGB normalized\n",
    "        r,g,b = x[:,0:1], x[:,1:2], x[:,2:3]\n",
    "        y = 0.299*r + 0.587*g + 0.114*b\n",
    "        return F.conv2d(y, self.weight, self.bias, padding=2)\n",
    "\n",
    "class RECCE(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.sobel_x = nn.Conv2d(1,1,3,padding=1,bias=False)\n",
    "        self.sobel_y = nn.Conv2d(1,1,3,padding=1,bias=False)\n",
    "        self.lap     = nn.Conv2d(1,1,3,padding=1,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.sobel_x.weight.copy_(torch.tensor([[[[-1,0,1],[-2,0,2],[-1,0,1]]]], dtype=torch.float32))\n",
    "            self.sobel_y.weight.copy_(torch.tensor([[[[-1,-2,-1],[0,0,0],[1,2,1]]]], dtype=torch.float32))\n",
    "            self.lap.weight.copy_(torch.tensor([[[[0,1,0],[1,-4,1],[0,1,0]]]], dtype=torch.float32))\n",
    "        for m in [self.sobel_x, self.sobel_y, self.lap]:\n",
    "            m.requires_grad_(False)\n",
    "\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "\n",
    "        self.quant   = torch.ao.quantization.QuantStub()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        r,g,b = x[:,0:1], x[:,1:2], x[:,2:3]\n",
    "        y = 0.299*r + 0.587*g + 0.114*b\n",
    "        sx = self.sobel_x(y)\n",
    "        sy = self.sobel_y(y)\n",
    "        lp = self.lap(y)\n",
    "        x6 = torch.cat([x, sx, sy, lp], dim=1)\n",
    "\n",
    "        x6_q = self.quant(x6)\n",
    "        out  = self.backbone(x6_q)\n",
    "        out  = self.dequant(out)\n",
    "        return out\n",
    "\n",
    "model = RECCE()\n",
    "model.load_state_dict(torch.load(\"./runs/recce_standard_fp32_final.pt\", map_location=\"cpu\"))\n",
    "print(\"Loaded pretrained FP32 RECCE.\")\n",
    "\n",
    "class SRMNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.srm = SRMConv2d()\n",
    "        self.head = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.head.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.head.fc = nn.Linear(self.head.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.srm(x)\n",
    "        return self.head(s)\n",
    "\n",
    "def make_xception(num_classes=2):\n",
    "    return timm.create_model(\"xception\", pretrained=True, num_classes=num_classes)\n",
    "\n",
    "def make_model(kind:str):\n",
    "    k = kind.lower()\n",
    "    if k == \"xception\": return make_xception(NUM_CLASSES).to(device)\n",
    "    if k == \"recce\":    return RECCE(NUM_CLASSES).to(device)\n",
    "    if k == \"srm\":      return SRMNet(NUM_CLASSES).to(device)\n",
    "    raise ValueError(\"kind must be one of: xception | recce | srm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f14d6",
   "metadata": {},
   "source": [
    "## Part 5 — Adversarial Attack & Robust Training Losses\n",
    "\n",
    "This section:\n",
    "- Implements `pgd_linf()` to generate ℓ∞ PGD adversarial examples:\n",
    "  - Works in normalized space.\n",
    "  - Uses multi-step gradient ascent on the loss.\n",
    "- Implements `trades_loss()`:\n",
    "  - Inner loop builds adversarial examples by minimizing KL divergence between clean and adversarial predictions.\n",
    "  - Final loss combines standard cross-entropy and a robustness KL penalty scaled by `TRADES_BETA`.\n",
    "\n",
    "These functions are later used for PGD-Adversarial Training and TRADES-based training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_linf(model, x, y, eps=PGD_EPS, alpha=PGD_ALPHA, steps=PGD_STEPS):\n",
    "    model.eval()\n",
    "    from contextlib import nullcontext\n",
    "    autocast_off = torch.cuda.amp.autocast(enabled=False)\n",
    "\n",
    "    eps_n, alpha_n = to_norm_eps(eps, alpha)\n",
    "    lo, hi = denorm_bounds()\n",
    "\n",
    "    x_adv = x.detach() + torch.empty_like(x).uniform_(-eps_n.max(), eps_n.max())\n",
    "    x_adv = x_adv.clamp(lo, hi)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad_(True)\n",
    "        with autocast_off:\n",
    "            logits = model(x_adv)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        g = torch.autograd.grad(loss, x_adv, only_inputs=True)[0]\n",
    "        x_adv = x_adv.detach() + alpha_n * torch.sign(g)\n",
    "        x_adv = torch.max(torch.min(x_adv, x + eps_n), x - eps_n)\n",
    "        x_adv = x_adv.clamp(lo, hi)\n",
    "    return x_adv.detach()\n",
    "\n",
    "\n",
    "def trades_loss(model, x, y, beta=TRADES_BETA, eps=PGD_EPS, alpha=PGD_ALPHA, steps=PGD_STEPS):\n",
    "    from contextlib import nullcontext\n",
    "    autocast_off = torch.cuda.amp.autocast(enabled=False)\n",
    "\n",
    "    model.train()\n",
    "    eps_n, alpha_n = to_norm_eps(eps, alpha)\n",
    "    lo, hi = denorm_bounds()\n",
    "\n",
    "    x_adv = x.detach() + 0.001*torch.randn_like(x)\n",
    "    x_adv = torch.max(torch.min(x_adv, x + eps_n), x - eps_n).clamp(lo, hi)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p = F.softmax(model(x), dim=1)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad_(True)\n",
    "        with autocast_off:\n",
    "            q_logits = model(x_adv)\n",
    "            loss_kl = F.kl_div(F.log_softmax(q_logits, dim=1), p, reduction='batchmean')\n",
    "        g = torch.autograd.grad(loss_kl, x_adv, only_inputs=True)[0]\n",
    "        x_adv = x_adv.detach() + alpha_n * torch.sign(g)\n",
    "        x_adv = torch.max(torch.min(x_adv, x + eps_n), x - eps_n).clamp(lo, hi)\n",
    "\n",
    "    logits_clean = model(x)\n",
    "    logits_adv   = model(x_adv)\n",
    "    loss_nll = F.cross_entropy(logits_clean, y)\n",
    "    loss_rob = F.kl_div(F.log_softmax(logits_adv,dim=1),\n",
    "                        F.softmax(logits_clean.detach(),dim=1),\n",
    "                        reduction='batchmean')\n",
    "    return loss_nll + beta * loss_rob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3efa9c6",
   "metadata": {},
   "source": [
    "## Part 6 — Evaluation Function and Base Training Loop\n",
    "\n",
    "This section:\n",
    "- Defines `evaluate(model, loader)`:\n",
    "  - Computes Accuracy, AUROC, AUPRC, and F1 on a given DataLoader.\n",
    "- Defines a general `train()` function:\n",
    "  - Supports three regimes:\n",
    "    - `\"standard\"` (clean training)\n",
    "    - `\"pgd_at\"` (PGD Adversarial Training)\n",
    "    - `\"trades\"` (TRADES robust training)\n",
    "  - Tracks validation metrics and keeps the best model based on AUROC.\n",
    "\n",
    "This is the core training/evaluation logic used throughout the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9b7ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    auroc, auprc, f1 = make_metrics()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            prob_fake = torch.softmax(logits, dim=1)[:,1]\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            total   += y.numel()\n",
    "            auroc.update(prob_fake, y)\n",
    "            auprc.update(prob_fake, y)\n",
    "            f1.update(pred, y)\n",
    "    return {\n",
    "        \"acc\": float(correct/total),\n",
    "        \"auroc\": float(auroc.compute().item()),\n",
    "        \"auprc\": float(auprc.compute().item()),\n",
    "        \"f1\": float(f1.compute().item())\n",
    "    }\n",
    "\n",
    "def train(model, regime=\"standard\", epochs=10, lr=3e-4, weight_decay=1e-4,\n",
    "          pgd_eps=PGD_EPS, pgd_alpha=PGD_ALPHA, pgd_steps=PGD_STEPS, trades_beta=TRADES_BETA):\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best, best_state = -1, None\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            if regime == \"standard\":\n",
    "                loss = F.cross_entropy(model(x), y)\n",
    "            elif regime == \"pgd_at\":\n",
    "                x_adv = pgd_linf(model, x, y, eps=pgd_eps, alpha=pgd_alpha, steps=pgd_steps)\n",
    "                loss  = F.cross_entropy(model(x_adv), y)\n",
    "            elif regime == \"trades\":\n",
    "                loss  = trades_loss(model, x, y, beta=trades_beta, eps=pgd_eps, alpha=pgd_alpha, steps=pgd_steps)\n",
    "            else:\n",
    "                raise ValueError(\"regime must be 'standard' | 'pgd_at' | 'trades'\")\n",
    "            loss.backward(); opt.step()\n",
    "            running += loss.item() * x.size(0)\n",
    "\n",
    "        val_m = evaluate(model, val_loader)\n",
    "        print(f\"Ep {ep:02d} | train_loss={(running/len(train_ds)):.4f} | \"\n",
    "              f\"val_acc={val_m['acc']:.3f} auroc={val_m['auroc']:.3f} auprc={val_m['auprc']:.3f} f1={val_m['f1']:.3f}\")\n",
    "        if val_m['auroc'] > best:\n",
    "            best = val_m['auroc']; best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7bd246",
   "metadata": {},
   "source": [
    "## Part 7 — Eager Quantization Utilities (Dynamic, Static, QAT)\n",
    "\n",
    "This section:\n",
    "- Defines:\n",
    "  - `model_size_mb(path)`: compute model size in MB.\n",
    "  - `latency_ms(model)`: approximate CPU inference latency.\n",
    "  - `save_state(tag, model)`: save weights and report size/latency.\n",
    "- Implements:\n",
    "  - `quantize_dynamic()`: dynamic (weights-only) INT8 quantization of Linear layers.\n",
    "  - `quantize_static()`: static post-training quantization with calibration.\n",
    "  - `qat_int8()`: a basic Quantization-Aware Training loop for ResNet-like models.\n",
    "\n",
    "These are early quantization utilities; later you use an FX-based version that is more advanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.ao.quantization as tq\n",
    "\n",
    "def model_size_mb(path): return os.path.getsize(path)/1e6\n",
    "\n",
    "def latency_ms(model, iters=50, bs=1):\n",
    "    m = copy.deepcopy(model).to('cpu').eval()\n",
    "    x = torch.randn(bs,3,IMSIZE,IMSIZE)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10): m(x)\n",
    "        t0=time.time()\n",
    "        for _ in range(iters): m(x)\n",
    "        t1=time.time()\n",
    "    return 1e3*(t1-t0)/iters\n",
    "\n",
    "def save_state(tag, model):\n",
    "    path = os.path.join(RUN_DIR, f\"{tag}.pt\")\n",
    "    torch.save(model.state_dict(), path)\n",
    "    return path, model_size_mb(path), latency_ms(model)\n",
    "\n",
    "def quantize_dynamic(model_cpu):\n",
    "    return tq.quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "def quantize_static(model_cpu, calib_loader, backend=\"fbgemm\"):\n",
    "    m = copy.deepcopy(model_cpu).to('cpu').eval()\n",
    "    m.qconfig = tq.get_default_qconfig(backend)\n",
    "    tq.prepare(m, inplace=True)\n",
    "    with torch.no_grad():\n",
    "        for x,_ in calib_loader:\n",
    "            m(x)\n",
    "    tq.convert(m, inplace=True)\n",
    "    return m\n",
    "\n",
    "def qat_int8(base_model, epochs=5, lr=1e-4, backend=\"fbgemm\"):\n",
    "    m = copy.deepcopy(base_model).to('cpu')\n",
    "    m.train()\n",
    "    m.qconfig = tq.get_default_qat_qconfig(backend)\n",
    "    tq.prepare_qat(m, inplace=True)\n",
    "    m = m.to(device)\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "    for ep in range(1, epochs+1):\n",
    "        m.train()\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            loss = F.cross_entropy(m(x), y)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "        print(f\"[QAT] epoch {ep:02d} done\")\n",
    "    m_cpu = copy.deepcopy(m).to('cpu').eval()\n",
    "    tq.convert(m_cpu, inplace=True)\n",
    "    return m_cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9ae2c",
   "metadata": {},
   "source": [
    "## Part 8 — RECCE-Specific Eager PTQ and QAT Utilities\n",
    "\n",
    "This section:\n",
    "- Defines `recce_static_ptq()`:\n",
    "  - Runs static post-training quantization on a RECCE model using its `QuantStub/DeQuantStub` structure and a calibration loader.\n",
    "- Defines `recce_qat()`:\n",
    "  - Runs QAT for RECCE on CPU/GPU:\n",
    "    - Prepares the model with `prepare_qat`.\n",
    "    - Fine-tunes for a given number of epochs.\n",
    "    - Converts to a final INT8 model.\n",
    "- Defines `save_and_report(tag, m)`:\n",
    "  - Saves a given model under `./runs`.\n",
    "  - Returns its path, size in MB, and CPU latency.\n",
    "\n",
    "These helpers are an earlier quantization path; later you add the FX-based quantization (more modern).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ac21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.ao.quantization as tq\n",
    "\n",
    "def recce_static_ptq(recce_model, calib_loader, backend=\"fbgemm\"):\n",
    "    m = recce_model.to('cpu').eval()\n",
    "    tq.backend = backend\n",
    "    m.qconfig = tq.get_default_qconfig(backend)\n",
    "    tq.prepare(m, inplace=True)\n",
    "    with torch.no_grad():\n",
    "        for x,_ in calib_loader:\n",
    "            m(x)\n",
    "    tq.convert(m, inplace=True)\n",
    "    return m\n",
    "\n",
    "def recce_qat(recce_model, train_loader, epochs=5, lr=1e-4, backend=\"fbgemm\", device=None):\n",
    "    dev = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    m = recce_model.to('cpu').train()\n",
    "    tq.backend = backend\n",
    "    m.qconfig = tq.get_default_qat_qconfig(backend)\n",
    "    torch.ao.quantization.prepare_qat(m, inplace=True)\n",
    "\n",
    "    m = m.to(dev)\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "    for ep in range(1, epochs+1):\n",
    "        m.train()\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(dev), y.to(dev)\n",
    "            loss = F.cross_entropy(m(x), y)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "        print(f\"[QAT] epoch {ep}/{epochs} done\")\n",
    "\n",
    "    m_cpu = m.to('cpu').eval()\n",
    "    torch.ao.quantization.convert(m_cpu, inplace=True)\n",
    "    return m_cpu\n",
    "\n",
    "def save_and_report(tag, m):\n",
    "    p = os.path.join(\"./runs\", f\"{tag}.pt\"); torch.save(m.state_dict(), p)\n",
    "    from pathlib import Path; size_mb = Path(p).stat().st_size/1e6\n",
    "    lat_ms = latency_ms(m)\n",
    "    return {\"path\": p, \"size_mb\": size_mb, \"lat_ms\": lat_ms}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ddb71",
   "metadata": {},
   "source": [
    "## Part 9 — Smoke Test: Single Forward Pass\n",
    "\n",
    "This section:\n",
    "- Creates a tiny DataLoader to grab a small batch.\n",
    "- Instantiates `model_dbg = make_model(\"recce\")`.\n",
    "- Runs a single forward pass and prints:\n",
    "  - Output shape (logits)\n",
    "  - Approximate forward time in milliseconds.\n",
    "\n",
    "Purpose: verify that the model compiles and runs end-to-end before heavy training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9bc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward OK. Logits shape: torch.Size([8, 2]) Time: 145.0 ms\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "\n",
    "tiny_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=8, shuffle=True, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "x,y = next(iter(tiny_loader))\n",
    "x,y = x.to(device), y.to(device)\n",
    "\n",
    "model_dbg = make_model(\"recce\")\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    logits = model_dbg(x)\n",
    "t1 = time.time()\n",
    "print(\"Forward OK. Logits shape:\", logits.shape, \"Time:\", round(1000*(t1-t0),1), \"ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9dda6",
   "metadata": {},
   "source": [
    "## Part 10 — DataLoader Configuration (Windows-Safe)\n",
    "\n",
    "This section:\n",
    "- Rebuilds `train_loader`, `val_loader`, and `test_loader` with:\n",
    "  - `num_workers = 0`\n",
    "  - `persistent_workers = False`\n",
    "  - `pin_memory = False`\n",
    "for maximum stability on Windows and inside some IDEs (e.g., VS Code Jupyter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "HAS_CUDA = torch.cuda.is_available()\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=min(32, BATCH), shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=False, persistent_workers=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=min(32, BATCH), shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=False, persistent_workers=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=min(32, BATCH), shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=False, persistent_workers=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6bfbd6",
   "metadata": {},
   "source": [
    "## Part 11 — Progress-Bar Training Loop (TQDM)\n",
    "\n",
    "This section:\n",
    "- Redefines `train()` to:\n",
    "  - Use a `tqdm` progress bar during training.\n",
    "  - Show batch-level loss live.\n",
    "- Supports the same training regimes (`standard`, `pgd_at`, `trades`).\n",
    "- Prints per-epoch summary with train loss and validation metrics.\n",
    "\n",
    "This is the user-friendly training loop you actually used to train RECCE for 8 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model, regime=\"standard\", epochs=3, lr=3e-4, weight_decay=1e-4,\n",
    "          pgd_eps=8/255, pgd_alpha=2/255, pgd_steps=10, trades_beta=6.0):\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best, best_state = -1, None\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        it = tqdm(train_loader, desc=f\"Epoch {ep}/{epochs} [{regime}]\", leave=False)\n",
    "        for x,y in it:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            if regime == \"standard\":\n",
    "                loss = F.cross_entropy(model(x), y)\n",
    "            elif regime == \"pgd_at\":\n",
    "                x_adv = pgd_linf(model, x, y, eps=pgd_eps, alpha=pgd_alpha, steps=min(pgd_steps, 2))\n",
    "                loss  = F.cross_entropy(model(x_adv), y)\n",
    "            elif regime == \"trades\":\n",
    "                loss  = trades_loss(model, x, y, beta=trades_beta, eps=pgd_eps, alpha=pgd_alpha, steps=min(pgd_steps, 2))\n",
    "            else:\n",
    "                raise ValueError(\"regime must be 'standard' | 'pgd_at' | 'trades'\")\n",
    "            loss.backward(); opt.step()\n",
    "            running += loss.item() * x.size(0)\n",
    "            it.set_postfix(loss=f\"{loss.item():.3f}\")\n",
    "        val_m = evaluate(model, val_loader)\n",
    "        print(f\"Ep {ep:02d} | train_loss={(running/len(train_ds)):.4f} | \"\n",
    "              f\"val_acc={val_m['acc']:.3f} auroc={val_m['auroc']:.3f} auprc={val_m['auprc']:.3f} f1={val_m['f1']:.3f}\")\n",
    "        if val_m['auroc'] > best:\n",
    "            best = val_m['auroc']; best_state = copy.deepcopy(model.state_dict())\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bedc5",
   "metadata": {},
   "source": [
    "## Part 12 — Debug Training and Initial 1-Epoch Run\n",
    "\n",
    "This section:\n",
    "- Defines `train_debug()`:\n",
    "  - Runs only a few batches per epoch (e.g., 5 batches).\n",
    "  - Prints detailed batch-level loss and validation metrics.\n",
    "  - Useful for debugging logic or data issues.\n",
    "- Then sets:\n",
    "  - `MODEL_KIND = \"recce\"`\n",
    "  - `REGIME = \"standard\"`\n",
    "  - `EPOCHS = 1`\n",
    "- Trains RECCE for 1 epoch as a quick sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ce5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99248bb0b8d94097b6db1786e77af1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 [standard]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 01 | train_loss=0.1193 | val_acc=0.952 auroc=0.992 auprc=0.992 f1=0.952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from itertools import islice\n",
    "\n",
    "def train_debug(model, regime=\"standard\", epochs=1, lr=3e-4, weight_decay=1e-4,\n",
    "                pgd_eps=PGD_EPS, pgd_alpha=PGD_ALPHA, pgd_steps=PGD_STEPS, trades_beta=TRADES_BETA):\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        print(f\"[start epoch {ep}/{epochs}] regime={regime}\", flush=True)\n",
    "\n",
    "        for b, (x,y) in enumerate(islice(train_loader, 5)):\n",
    "            if b == 0: print(\" first batch pulled\", flush=True)\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            if regime == \"standard\":\n",
    "                loss = F.cross_entropy(model(x), y)\n",
    "            elif regime == \"pgd_at\":\n",
    "                x_adv = pgd_linf(model, x, y, eps=pgd_eps, alpha=pgd_alpha, steps=2)\n",
    "                loss  = F.cross_entropy(model(x_adv), y)\n",
    "            elif regime == \"trades\":\n",
    "                loss  = trades_loss(model, x, y, beta=trades_beta, eps=pgd_eps, alpha=pgd_alpha, steps=2)\n",
    "            else:\n",
    "                raise ValueError(\"regime must be 'standard' | 'pgd_at' | 'trades'\")\n",
    "\n",
    "            loss.backward(); opt.step()\n",
    "            running += loss.item() * x.size(0)\n",
    "            print(f\"  batch {b}: loss={loss.item():.4f}\", flush=True)\n",
    "\n",
    "        val_m = evaluate(model, val_loader)\n",
    "        print(f\"[end epoch {ep}] train_loss={(running/(5*train_loader.batch_size)):.4f} | \"\n",
    "              f\"val_acc={val_m['acc']:.3f} auroc={val_m['auroc']:.3f}\", flush=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL_KIND = \"recce\"\n",
    "REGIME     = \"standard\"\n",
    "EPOCHS     = 1\n",
    "\n",
    "model = make_model(MODEL_KIND)\n",
    "model = train(model, regime=REGIME, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ba09c",
   "metadata": {},
   "source": [
    "## Part 13 — Main FP32 Training (8 Epochs, RECCE)\n",
    "\n",
    "This section:\n",
    "- Configures:\n",
    "  - `MODEL_KIND = \"recce\"`\n",
    "  - `REGIME = \"standard\"`\n",
    "  - `EPOCHS = 8`\n",
    "- Recreates the RECCE model and trains it for 8 epochs.\n",
    "- This is the primary FP32 baseline you later quantize and evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aead1197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8d8be80fa841f2a698477f3a2e4c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/8 [standard]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 01 | train_loss=0.1196 | val_acc=0.953 auroc=0.994 auprc=0.994 f1=0.952\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1528b5660a1a4b71b0f7ca8e2313319a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/8 [standard]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 02 | train_loss=0.0712 | val_acc=0.955 auroc=0.994 auprc=0.994 f1=0.954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dc1eb60bac4186b4afe8675996fea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/8 [standard]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 03 | train_loss=0.0599 | val_acc=0.967 auroc=0.996 auprc=0.996 f1=0.967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f58f0c78c1741dc9596b48767833342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/8 [standard]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 04 | train_loss=0.0528 | val_acc=0.959 auroc=0.996 auprc=0.996 f1=0.958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cb7d517ef84b438935035c4823f096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/8 [standard]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 05 | train_loss=0.0465 | val_acc=0.967 auroc=0.996 auprc=0.996 f1=0.967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142905053b164541a88a921a4639a566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/8 [standard]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 06 | train_loss=0.0442 | val_acc=0.974 auroc=0.997 auprc=0.997 f1=0.974\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a151c05cb2c8492ca9bd622a04ff0fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/8 [standard]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 07 | train_loss=0.0409 | val_acc=0.954 auroc=0.996 auprc=0.996 f1=0.952\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1041110dfd422b83affd023a383798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/8 [standard]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 08 | train_loss=0.0395 | val_acc=0.973 auroc=0.997 auprc=0.997 f1=0.973\n"
     ]
    }
   ],
   "source": [
    "MODEL_KIND = \"recce\"\n",
    "REGIME     = \"standard\"\n",
    "EPOCHS     = 8\n",
    "\n",
    "model = make_model(MODEL_KIND)\n",
    "model = train(model, regime=REGIME, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7042dd2",
   "metadata": {},
   "source": [
    "## Part 14 — Evaluation Helpers for FP32 and INT8 (CPU Version, Eager)\n",
    "\n",
    "This section:\n",
    "- Defines `evaluate_fp32()`:\n",
    "  - Evaluates FP32 models on GPU (or CPU if no CUDA) with Accuracy, AUROC, AUPRC, and F1.\n",
    "- Defines `evaluate_int8_cpu()`:\n",
    "  - Evaluates quantized INT8 models on CPU (for static/dynamic/QAT) using TorchMetrics.\n",
    "\n",
    "These were the first evaluation helpers for your quantized models; later updated for FX-based quantization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fp32(model, loader) -> Dict[str, float]:\n",
    "    \"\"\"Use this for FP32 models on GPU (or CPU if no CUDA).\"\"\"\n",
    "    model = model.to(device).eval()\n",
    "    auroc, auprc, f1 = make_metrics()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            prob_fake = torch.softmax(logits, dim=1)[:,1]\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            total   += y.numel()\n",
    "            auroc.update(prob_fake, y)\n",
    "            auprc.update(prob_fake, y)\n",
    "            f1.update(pred, y)\n",
    "    return {\n",
    "        \"acc\": float(correct/total),\n",
    "        \"auroc\": float(auroc.compute().item()),\n",
    "        \"auprc\": float(auprc.compute().item()),\n",
    "        \"f1\": float(f1.compute().item())\n",
    "    }\n",
    "\n",
    "def evaluate_int8_cpu(model, loader) -> Dict[str, float]:\n",
    "    \"\"\"Use this for STATIC/QAT/DYNAMIC INT8 models (CPU-only).\"\"\"\n",
    "    model = copy.deepcopy(model).to(\"cpu\").eval()\n",
    "    auroc = BinaryAUROC().to(\"cpu\")\n",
    "    auprc = BinaryAveragePrecision().to(\"cpu\")\n",
    "    f1    = BinaryF1Score().to(\"cpu\")\n",
    "\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            logits = model(x)\n",
    "            prob_fake = torch.softmax(logits, dim=1)[:,1]\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            total   += y.numel()\n",
    "            auroc.update(prob_fake.cpu(), y.cpu())\n",
    "            auprc.update(prob_fake.cpu(), y.cpu())\n",
    "            f1.update(pred.cpu(), y.cpu())\n",
    "    return {\n",
    "        \"acc\": float(correct/total),\n",
    "        \"auroc\": float(auroc.compute().item()),\n",
    "        \"auprc\": float(auprc.compute().item()),\n",
    "        \"f1\": float(f1.compute().item())\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4494d2",
   "metadata": {},
   "source": [
    "## Part 15 — FX Graph-Mode Quantization Helpers for RECCE\n",
    "\n",
    "This section:\n",
    "- Imports FX-based quantization utilities: `prepare_fx`, `convert_fx`, `prepare_qat_fx`.\n",
    "- Redefines:\n",
    "  - `evaluate_fp32()` and `evaluate_int8_cpu()` (now consistent with FX flow).\n",
    "- Implements:\n",
    "  - `recce_fx_static_ptq()`:\n",
    "    - Uses FX `prepare_fx` and `convert_fx` with an example input and calibration loader.\n",
    "  - `recce_fx_qat()`:\n",
    "    - Uses `prepare_qat_fx` to inject fake-quant nodes.\n",
    "    - Fine-tunes with QAT for a small number of epochs.\n",
    "    - Converts to a fully quantized INT8 model.\n",
    "\n",
    "This is the “advanced” quantization path compatible with PyTorch 2.9+ deprecations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a475a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.ao.quantization as tq\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, prepare_qat_fx\n",
    "\n",
    "def evaluate_fp32(model, loader) -> Dict[str, float]:\n",
    "    model = model.to(device).eval()\n",
    "    auroc, auprc, f1 = make_metrics()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            prob_fake = torch.softmax(logits, dim=1)[:,1]\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            total   += y.numel()\n",
    "            auroc.update(prob_fake, y)\n",
    "            auprc.update(prob_fake, y)\n",
    "            f1.update(pred, y)\n",
    "    return {\n",
    "        \"acc\": float(correct/total),\n",
    "        \"auroc\": float(auroc.compute().item()),\n",
    "        \"auprc\": float(auprc.compute().item()),\n",
    "        \"f1\": float(f1.compute().item())\n",
    "    }\n",
    "\n",
    "def evaluate_int8_cpu(model, loader) -> Dict[str, float]:\n",
    "    m = copy.deepcopy(model).to(\"cpu\").eval()\n",
    "    auroc = BinaryAUROC().to(\"cpu\")\n",
    "    auprc = BinaryAveragePrecision().to(\"cpu\")\n",
    "    f1    = BinaryF1Score().to(\"cpu\")\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            logits = m(x)\n",
    "            prob_fake = torch.softmax(logits, dim=1)[:,1]\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            total   += y.numel()\n",
    "            auroc.update(prob_fake.cpu(), y.cpu())\n",
    "            auprc.update(prob_fake.cpu(), y.cpu())\n",
    "            f1.update(pred.cpu(), y.cpu())\n",
    "    return {\n",
    "        \"acc\": float(correct/total),\n",
    "        \"auroc\": float(auroc.compute().item()),\n",
    "        \"auprc\": float(auprc.compute().item()),\n",
    "        \"f1\": float(f1.compute().item())\n",
    "    }\n",
    "\n",
    "def recce_fx_static_ptq(model_fp32, calib_loader, backend=\"fbgemm\"):\n",
    "    m = copy.deepcopy(model_fp32).to(\"cpu\").eval()\n",
    "    tq.backend = backend\n",
    "    qconfig = tq.get_default_qconfig(backend)\n",
    "    qconfig_dict = {\"\": qconfig}\n",
    "\n",
    "    example_x, _ = next(iter(calib_loader))\n",
    "    prepared = prepare_fx(m, qconfig_dict, example_inputs=example_x)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,_ in calib_loader:\n",
    "            prepared(x)\n",
    "    quantized = convert_fx(prepared)\n",
    "    return quantized\n",
    "\n",
    "def recce_fx_qat(model_fp32, train_loader, epochs=5, lr=1e-4,\n",
    "                 backend=\"fbgemm\", device=None):\n",
    "    dev = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    base = copy.deepcopy(model_fp32).to(\"cpu\").train()\n",
    "    tq.backend = backend\n",
    "    qconfig = tq.get_default_qat_qconfig(backend)\n",
    "    qconfig_dict = {\"\": qconfig}\n",
    "\n",
    "    example_x, _ = next(iter(train_loader))\n",
    "    prepared = prepare_qat_fx(base, qconfig_dict, example_inputs=example_x).to(dev)\n",
    "\n",
    "    opt = torch.optim.AdamW(prepared.parameters(), lr=lr)\n",
    "    for ep in range(1, epochs+1):\n",
    "        prepared.train()\n",
    "        running = 0.0\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(dev), y.to(dev)\n",
    "            logits = prepared(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            running += loss.item() * x.size(0)\n",
    "        print(f\"[FX-QAT] epoch {ep}/{epochs} done, train_loss={running/len(train_ds):.4f}\")\n",
    "\n",
    "    prepared_cpu = prepared.to(\"cpu\").eval()\n",
    "    quantized = convert_fx(prepared_cpu)\n",
    "    return quantized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236199d9",
   "metadata": {},
   "source": [
    "## Part 16 — Quantization Suite (FP32 vs Dynamic vs Static-FX vs QAT-FX)\n",
    "\n",
    "This section:\n",
    "- Asserts that all necessary symbols from earlier parts exist.\n",
    "- Configures:\n",
    "  - `RUN_DIR`, `MODEL_TAG`, number of calibration samples, and QAT epochs.\n",
    "- Evaluates and saves:\n",
    "  1. **FP32 baseline** (`recce_standard_fp32_final`)\n",
    "  2. **INT8 Dynamic** (weights-only)\n",
    "  3. **INT8 Static FX** (calibrated)\n",
    "  4. **INT8 QAT FX** (fine-tuned with fake-quant)\n",
    "- Computes:\n",
    "  - Accuracy, AUROC, AUPRC, F1\n",
    "  - Model size (MB)\n",
    "  - CPU latency (ms)\n",
    "- Displays a summary table and saves a JSON file `*_quant_summary.json` with all metrics and model info.\n",
    "\n",
    "This is your main **quantization comparison** section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] evaluating baseline…\n",
      "[INT8-Dynamic] quantizing + evaluating…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_19916\\1405302616.py:39: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  dyn = torch.ao.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INT8-Static FX] building calibration loader & evaluating…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_19916\\2541815151.py:61: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared = prepare_fx(m, qconfig_dict, example_inputs=example_x)\n",
      "c:\\Users\\admin\\Github\\DFDetectAdversAttack\\.venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  prepared = prepare(\n",
      "c:\\Users\\admin\\Github\\DFDetectAdversAttack\\.venv\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_19916\\2541815151.py:67: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized = convert_fx(prepared)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INT8-QAT FX] fine-tuning for 5 epochs + evaluating…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_19916\\2541815151.py:81: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared = prepare_qat_fx(base, qconfig_dict, example_inputs=example_x).to(dev)\n",
      "c:\\Users\\admin\\Github\\DFDetectAdversAttack\\.venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  prepared = prepare(\n",
      "c:\\Users\\admin\\Github\\DFDetectAdversAttack\\.venv\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FX-QAT] epoch 1/5 done, train_loss=0.0371\n",
      "[FX-QAT] epoch 2/5 done, train_loss=0.0323\n",
      "[FX-QAT] epoch 3/5 done, train_loss=0.0308\n",
      "[FX-QAT] epoch 4/5 done, train_loss=0.0286\n",
      "[FX-QAT] epoch 5/5 done, train_loss=0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_19916\\2541815151.py:96: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized = convert_fx(prepared_cpu)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_78017\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_78017_level0_col0\" class=\"col_heading level0 col0\" >Version</th>\n",
       "      <th id=\"T_78017_level0_col1\" class=\"col_heading level0 col1\" >Acc</th>\n",
       "      <th id=\"T_78017_level0_col2\" class=\"col_heading level0 col2\" >AUROC</th>\n",
       "      <th id=\"T_78017_level0_col3\" class=\"col_heading level0 col3\" >AUPRC</th>\n",
       "      <th id=\"T_78017_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_78017_level0_col5\" class=\"col_heading level0 col5\" >Size (MB)</th>\n",
       "      <th id=\"T_78017_level0_col6\" class=\"col_heading level0 col6\" >CPU Latency (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_78017_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_78017_row0_col0\" class=\"data row0 col0\" >FP32</td>\n",
       "      <td id=\"T_78017_row0_col1\" class=\"data row0 col1\" >0.901</td>\n",
       "      <td id=\"T_78017_row0_col2\" class=\"data row0 col2\" >0.960</td>\n",
       "      <td id=\"T_78017_row0_col3\" class=\"data row0 col3\" >0.969</td>\n",
       "      <td id=\"T_78017_row0_col4\" class=\"data row0 col4\" >0.894</td>\n",
       "      <td id=\"T_78017_row0_col5\" class=\"data row0 col5\" >44.8</td>\n",
       "      <td id=\"T_78017_row0_col6\" class=\"data row0 col6\" >21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_78017_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_78017_row1_col0\" class=\"data row1 col0\" >INT8-Dynamic</td>\n",
       "      <td id=\"T_78017_row1_col1\" class=\"data row1 col1\" >0.901</td>\n",
       "      <td id=\"T_78017_row1_col2\" class=\"data row1 col2\" >0.960</td>\n",
       "      <td id=\"T_78017_row1_col3\" class=\"data row1 col3\" >0.969</td>\n",
       "      <td id=\"T_78017_row1_col4\" class=\"data row1 col4\" >0.894</td>\n",
       "      <td id=\"T_78017_row1_col5\" class=\"data row1 col5\" >44.8</td>\n",
       "      <td id=\"T_78017_row1_col6\" class=\"data row1 col6\" >23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_78017_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_78017_row2_col0\" class=\"data row2 col0\" >INT8-StaticFX</td>\n",
       "      <td id=\"T_78017_row2_col1\" class=\"data row2 col1\" >0.899</td>\n",
       "      <td id=\"T_78017_row2_col2\" class=\"data row2 col2\" >0.960</td>\n",
       "      <td id=\"T_78017_row2_col3\" class=\"data row2 col3\" >0.968</td>\n",
       "      <td id=\"T_78017_row2_col4\" class=\"data row2 col4\" >0.891</td>\n",
       "      <td id=\"T_78017_row2_col5\" class=\"data row2 col5\" >11.3</td>\n",
       "      <td id=\"T_78017_row2_col6\" class=\"data row2 col6\" >11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_78017_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_78017_row3_col0\" class=\"data row3 col0\" >INT8-QATFX</td>\n",
       "      <td id=\"T_78017_row3_col1\" class=\"data row3 col1\" >0.888</td>\n",
       "      <td id=\"T_78017_row3_col2\" class=\"data row3 col2\" >0.967</td>\n",
       "      <td id=\"T_78017_row3_col3\" class=\"data row3 col3\" >0.974</td>\n",
       "      <td id=\"T_78017_row3_col4\" class=\"data row3 col4\" >0.875</td>\n",
       "      <td id=\"T_78017_row3_col5\" class=\"data row3 col5\" >11.3</td>\n",
       "      <td id=\"T_78017_row3_col6\" class=\"data row3 col6\" >12.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x289b5640aa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON: ./runs\\recce_standard_quant_summary.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, copy, numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "needed = [\"model\",\"train_ds\",\"train_loader\",\"test_loader\",\"device\",\n",
    "          \"evaluate_fp32\",\"evaluate_int8_cpu\",\"recce_fx_static_ptq\",\"recce_fx_qat\",\"latency_ms\"]\n",
    "missing = [n for n in needed if n not in globals()]\n",
    "assert not missing, f\"Missing in notebook: {missing}\"\n",
    "\n",
    "RUN_DIR = \"./runs\"; os.makedirs(RUN_DIR, exist_ok=True)\n",
    "MODEL_TAG = \"recce_standard\"\n",
    "CALIB_SAMPLES = min(1024, len(train_ds))\n",
    "QAT_EPOCHS = 5\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "def save_and_report(tag, m):\n",
    "    p = os.path.join(RUN_DIR, f\"{tag}.pt\")\n",
    "    torch.save(m.state_dict(), p)\n",
    "    size_mb = Path(p).stat().st_size / 1e6\n",
    "    lat_ms = latency_ms(m)\n",
    "    return {\"path\": p, \"size_mb\": size_mb, \"lat_ms\": lat_ms}\n",
    "\n",
    "print(\"[FP32] evaluating baseline…\")\n",
    "fp32_metrics = evaluate_fp32(model, test_loader)\n",
    "fp32_info    = save_and_report(f\"{MODEL_TAG}_fp32_final\",\n",
    "                               copy.deepcopy(model).to('cpu').eval())\n",
    "\n",
    "print(\"[INT8-Dynamic] quantizing + evaluating…\")\n",
    "dyn = torch.ao.quantization.quantize_dynamic(\n",
    "    copy.deepcopy(model).to('cpu').eval(),\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "dyn_metrics = evaluate_int8_cpu(dyn, test_loader)\n",
    "dyn_info    = save_and_report(f\"{MODEL_TAG}_int8_dynamic\", dyn)\n",
    "\n",
    "print(\"[INT8-Static FX] building calibration loader & evaluating…\")\n",
    "calib_idx = np.random.choice(len(train_ds), size=CALIB_SAMPLES, replace=False)\n",
    "calib_loader = DataLoader(Subset(train_ds, calib_idx), batch_size=64, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=False, persistent_workers=False)\n",
    "\n",
    "st = recce_fx_static_ptq(model, calib_loader)\n",
    "st_metrics = evaluate_int8_cpu(st, test_loader)\n",
    "st_info    = save_and_report(f\"{MODEL_TAG}_int8_static_fx\", st)\n",
    "\n",
    "print(f\"[INT8-QAT FX] fine-tuning for {QAT_EPOCHS} epochs + evaluating…\")\n",
    "qatm = recce_fx_qat(model, train_loader, epochs=QAT_EPOCHS, device=device)\n",
    "qat_metrics = evaluate_int8_cpu(qatm, test_loader)\n",
    "qat_info    = save_and_report(f\"{MODEL_TAG}_int8_qat_fx\", qatm)\n",
    "\n",
    "rows = [\n",
    "    [\"FP32\",          fp32_metrics.get(\"acc\"), fp32_metrics.get(\"auroc\"), fp32_metrics.get(\"auprc\"), fp32_metrics.get(\"f1\"),\n",
    "                      fp32_info[\"size_mb\"], fp32_info[\"lat_ms\"]],\n",
    "    [\"INT8-Dynamic\",  dyn_metrics.get(\"acc\"), dyn_metrics.get(\"auroc\"), dyn_metrics.get(\"auprc\"), dyn_metrics.get(\"f1\"),\n",
    "                      dyn_info[\"size_mb\"], dyn_info[\"lat_ms\"]],\n",
    "    [\"INT8-StaticFX\", st_metrics.get(\"acc\"),  st_metrics.get(\"auroc\"),  st_metrics.get(\"auprc\"),  st_metrics.get(\"f1\"),\n",
    "                      st_info[\"size_mb\"], st_info[\"lat_ms\"]],\n",
    "    [\"INT8-QATFX\",    qat_metrics.get(\"acc\"), qat_metrics.get(\"auroc\"), qat_metrics.get(\"auprc\"), qat_metrics.get(\"f1\"),\n",
    "                      qat_info[\"size_mb\"], qat_info[\"lat_ms\"]],\n",
    "]\n",
    "df = pd.DataFrame(rows, columns=[\"Version\",\"Acc\",\"AUROC\",\"AUPRC\",\"F1\",\"Size (MB)\",\"CPU Latency (ms)\"])\n",
    "display(df.style.format({\"Acc\":\"{:.3f}\",\"AUROC\":\"{:.3f}\",\"AUPRC\":\"{:.3f}\",\"F1\":\"{:.3f}\",\n",
    "                         \"Size (MB)\":\"{:.1f}\",\"CPU Latency (ms)\":\"{:.1f}\"}))\n",
    "\n",
    "summary = {\n",
    "    \"model_tag\": MODEL_TAG,\n",
    "    \"fp32\":           {\"metrics\": fp32_metrics, **fp32_info},\n",
    "    \"int8_dynamic\":   {\"metrics\": dyn_metrics,  **dyn_info},\n",
    "    \"int8_static_fx\": {\"metrics\": st_metrics,   **st_info},\n",
    "    \"int8_qat_fx\":    {\"metrics\": qat_metrics,  **qat_info},\n",
    "}\n",
    "with open(os.path.join(RUN_DIR, f\"{MODEL_TAG}_quant_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Saved JSON:\", os.path.join(RUN_DIR, f\"{MODEL_TAG}_quant_summary.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a391af8",
   "metadata": {},
   "source": [
    "## Part 17 — PGD Robustness Evaluation Helper\n",
    "\n",
    "This section:\n",
    "- Defines `evaluate_under_pgd()`:\n",
    "  - For each batch in a loader:\n",
    "    - Generates PGD adversarial examples (white-box).\n",
    "    - Evaluates model on these adversarial images.\n",
    "  - Returns Accuracy, AUROC, AUPRC, and F1 under PGD-10.\n",
    "\n",
    "Used in the robustness section to measure how well models survive strong adversarial attacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b3e5055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate_under_pgd(model, loader, steps=10, eps=PGD_EPS, alpha=PGD_ALPHA):\n",
    "    \"\"\"\n",
    "    Evaluate a model under white-box PGD (L_inf) on a given loader.\n",
    "    Returns the same metric dict structure as `evaluate`.\n",
    "    \"\"\"\n",
    "    model = model.to(device).eval()\n",
    "    auroc, auprc, f1 = make_metrics()\n",
    "    correct = total = 0\n",
    "\n",
    "    for x,y in tqdm(loader, desc=f\"PGD-{steps} eval\", leave=False):\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        x_adv = pgd_linf(model, x, y, eps=eps, alpha=alpha, steps=steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(x_adv)\n",
    "            prob_fake = torch.softmax(logits, dim=1)[:,1]\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total   += y.numel()\n",
    "            auroc.update(prob_fake, y)\n",
    "            auprc.update(prob_fake, y)\n",
    "            f1.update(pred, y)\n",
    "\n",
    "    return {\n",
    "        \"acc\":  float(correct/total),\n",
    "        \"auroc\": float(auroc.compute().item()),\n",
    "        \"auprc\": float(auprc.compute().item()),\n",
    "        \"f1\":    float(f1.compute().item())\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6e3d3",
   "metadata": {},
   "source": [
    "## Part 18 — Robustness & Adversarial Evaluation (FP32 & INT8)\n",
    "\n",
    "This section runs the full robustness study:\n",
    "- **Sanity checks & config**:\n",
    "  - Ensures all required symbols exist.\n",
    "  - Sets `MODEL_TAG`, `ROBUST_EPOCHS`, and flags:\n",
    "    - `DO_TRAIN_PGD_AT`, `DO_TRAIN_TRADES`, `DO_AUTOATTACK`.\n",
    "- **AutoAttack (optional)**:\n",
    "  - Sets up AutoAttack if enabled.\n",
    "- **JPEG robustness**:\n",
    "  - Implements JPEG compression for:\n",
    "    - FP32 models (`eval_jpeg_fp32`)\n",
    "    - INT8 models (`eval_jpeg_int8`)\n",
    "- **Evaluation helpers**:\n",
    "  - `eval_all_fp32(...)`:\n",
    "    - Clean metrics\n",
    "    - PGD-10 metrics\n",
    "    - Optional AutoAttack metrics\n",
    "    - JPEG robustness at qualities in `JPEG_QUALITIES`\n",
    "  - `eval_all_int8(...)`:\n",
    "    - Clean + JPEG robustness for INT8 models\n",
    "- **Models evaluated**:\n",
    "  - FP32 baseline (`*_fp32_standard`)\n",
    "  - FP32 PGD-AT (`*_fp32_pgd_at`) — if enabled\n",
    "  - FP32 TRADES (`*_fp32_trades`) — if enabled\n",
    "  - INT8 Dynamic, Static-FX, and QAT-FX variants (baseline + robust versions where available)\n",
    "- **Outputs**:\n",
    "  - A Pandas DataFrame `df` summarizing all models and robustness metrics.\n",
    "  - Saved to:\n",
    "    - `*_robust_summary.json`\n",
    "    - `*_robust_summary.csv`\n",
    "\n",
    "This section gives you the final story:\n",
    "how FP32 vs INT8 and Standard vs PGD-AT vs TRADES behave under attacks and compression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d5ceaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e244c497fcaf4986a2b25f4b48097a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD-10 eval:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_19916\\2103913432.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  autocast_off = torch.cuda.amp.autocast(enabled=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43f3421992d4be2933728fcaf0bc934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/8 [pgd_at]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 01 | train_loss=0.6946 | val_acc=0.498 auroc=0.611 auprc=0.591 f1=0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47de58af3eee400fb4b29cee018e93fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/8 [pgd_at]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 02 | train_loss=0.6931 | val_acc=0.502 auroc=0.500 auprc=0.502 f1=0.668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4922659ab74c788ae736749b419f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/8 [pgd_at]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 03 | train_loss=0.6932 | val_acc=0.502 auroc=0.500 auprc=0.502 f1=0.668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34705126c2cc4ad3ae250538ffd324d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/8 [pgd_at]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 04 | train_loss=0.6932 | val_acc=0.498 auroc=0.500 auprc=0.502 f1=0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b87736e4b94e88ab040599a0588d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/8 [pgd_at]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 05 | train_loss=0.6932 | val_acc=0.498 auroc=0.500 auprc=0.502 f1=0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4c2aaff6d44b58bf935f492579471f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/8 [pgd_at]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 06 | train_loss=0.6932 | val_acc=0.502 auroc=0.500 auprc=0.502 f1=0.668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c50a16c42014643b7a51109dc4e48f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/8 [pgd_at]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 07 | train_loss=0.6932 | val_acc=0.502 auroc=0.500 auprc=0.502 f1=0.668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63719cbdfe454752ada6017e29606997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/8 [pgd_at]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 08 | train_loss=0.6932 | val_acc=0.502 auroc=0.500 auprc=0.502 f1=0.668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6755f443a2a45638354f245937594ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD-10 eval:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31120a6c51ed4f9a960352e6c55db81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/8 [trades]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_19916\\2103913432.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  autocast_off = torch.cuda.amp.autocast(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 01 | train_loss=0.8847 | val_acc=0.904 auroc=0.968 auprc=0.968 f1=0.907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74a114de16a4c2a868156c707b8474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/8 [trades]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 02 | train_loss=1.0119 | val_acc=0.888 auroc=0.975 auprc=0.974 f1=0.880\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7078db8aeccb4e69b5d809e3279e847a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/8 [trades]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 03 | train_loss=1.0155 | val_acc=0.890 auroc=0.981 auprc=0.980 f1=0.881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fd9f73b0504ab7b08bf4d5acb26c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/8 [trades]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 04 | train_loss=0.9843 | val_acc=0.930 auroc=0.984 auprc=0.984 f1=0.929\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5088eb76bf4840978b7ca7bf4bd1dfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/8 [trades]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 05 | train_loss=0.9640 | val_acc=0.905 auroc=0.974 auprc=0.977 f1=0.899\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662ef75571ca4259a0e5f831791825b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/8 [trades]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 06 | train_loss=0.9378 | val_acc=0.903 auroc=0.987 auprc=0.987 f1=0.895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d043f969d6274d4580ffb0f477fe38f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/8 [trades]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 07 | train_loss=0.9238 | val_acc=0.941 auroc=0.992 auprc=0.992 f1=0.940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3999eb15260445c38da61991533e602a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/8 [trades]:   0%|          | 0/4376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 08 | train_loss=0.9039 | val_acc=0.918 auroc=0.990 auprc=0.989 f1=0.913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d09f4ed40e245a79402ec73bec4114f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD-10 eval:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_19916\\1197001126.py:22: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  return tq.quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dba34\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dba34_level0_col0\" class=\"col_heading level0 col0\" >ModelTag</th>\n",
       "      <th id=\"T_dba34_level0_col1\" class=\"col_heading level0 col1\" >clean_acc</th>\n",
       "      <th id=\"T_dba34_level0_col2\" class=\"col_heading level0 col2\" >clean_auroc</th>\n",
       "      <th id=\"T_dba34_level0_col3\" class=\"col_heading level0 col3\" >clean_auprc</th>\n",
       "      <th id=\"T_dba34_level0_col4\" class=\"col_heading level0 col4\" >clean_f1</th>\n",
       "      <th id=\"T_dba34_level0_col5\" class=\"col_heading level0 col5\" >pgd_acc</th>\n",
       "      <th id=\"T_dba34_level0_col6\" class=\"col_heading level0 col6\" >pgd_auroc</th>\n",
       "      <th id=\"T_dba34_level0_col7\" class=\"col_heading level0 col7\" >pgd_auprc</th>\n",
       "      <th id=\"T_dba34_level0_col8\" class=\"col_heading level0 col8\" >pgd_f1</th>\n",
       "      <th id=\"T_dba34_level0_col9\" class=\"col_heading level0 col9\" >jpeg90_acc</th>\n",
       "      <th id=\"T_dba34_level0_col10\" class=\"col_heading level0 col10\" >jpeg90_auroc</th>\n",
       "      <th id=\"T_dba34_level0_col11\" class=\"col_heading level0 col11\" >jpeg90_auprc</th>\n",
       "      <th id=\"T_dba34_level0_col12\" class=\"col_heading level0 col12\" >jpeg90_f1</th>\n",
       "      <th id=\"T_dba34_level0_col13\" class=\"col_heading level0 col13\" >jpeg70_acc</th>\n",
       "      <th id=\"T_dba34_level0_col14\" class=\"col_heading level0 col14\" >jpeg70_auroc</th>\n",
       "      <th id=\"T_dba34_level0_col15\" class=\"col_heading level0 col15\" >jpeg70_auprc</th>\n",
       "      <th id=\"T_dba34_level0_col16\" class=\"col_heading level0 col16\" >jpeg70_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dba34_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_dba34_row0_col0\" class=\"data row0 col0\" >recce_fp32_standard</td>\n",
       "      <td id=\"T_dba34_row0_col1\" class=\"data row0 col1\" >0.901</td>\n",
       "      <td id=\"T_dba34_row0_col2\" class=\"data row0 col2\" >0.960</td>\n",
       "      <td id=\"T_dba34_row0_col3\" class=\"data row0 col3\" >0.969</td>\n",
       "      <td id=\"T_dba34_row0_col4\" class=\"data row0 col4\" >0.894</td>\n",
       "      <td id=\"T_dba34_row0_col5\" class=\"data row0 col5\" >0.000</td>\n",
       "      <td id=\"T_dba34_row0_col6\" class=\"data row0 col6\" >0.000</td>\n",
       "      <td id=\"T_dba34_row0_col7\" class=\"data row0 col7\" >0.304</td>\n",
       "      <td id=\"T_dba34_row0_col8\" class=\"data row0 col8\" >0.000</td>\n",
       "      <td id=\"T_dba34_row0_col9\" class=\"data row0 col9\" >0.900</td>\n",
       "      <td id=\"T_dba34_row0_col10\" class=\"data row0 col10\" >0.960</td>\n",
       "      <td id=\"T_dba34_row0_col11\" class=\"data row0 col11\" >0.969</td>\n",
       "      <td id=\"T_dba34_row0_col12\" class=\"data row0 col12\" >0.892</td>\n",
       "      <td id=\"T_dba34_row0_col13\" class=\"data row0 col13\" >0.896</td>\n",
       "      <td id=\"T_dba34_row0_col14\" class=\"data row0 col14\" >0.959</td>\n",
       "      <td id=\"T_dba34_row0_col15\" class=\"data row0 col15\" >0.968</td>\n",
       "      <td id=\"T_dba34_row0_col16\" class=\"data row0 col16\" >0.887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dba34_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_dba34_row1_col0\" class=\"data row1 col0\" >recce_fp32_pgd_at</td>\n",
       "      <td id=\"T_dba34_row1_col1\" class=\"data row1 col1\" >0.504</td>\n",
       "      <td id=\"T_dba34_row1_col2\" class=\"data row1 col2\" >0.565</td>\n",
       "      <td id=\"T_dba34_row1_col3\" class=\"data row1 col3\" >0.558</td>\n",
       "      <td id=\"T_dba34_row1_col4\" class=\"data row1 col4\" >0.000</td>\n",
       "      <td id=\"T_dba34_row1_col5\" class=\"data row1 col5\" >0.504</td>\n",
       "      <td id=\"T_dba34_row1_col6\" class=\"data row1 col6\" >0.079</td>\n",
       "      <td id=\"T_dba34_row1_col7\" class=\"data row1 col7\" >0.318</td>\n",
       "      <td id=\"T_dba34_row1_col8\" class=\"data row1 col8\" >0.000</td>\n",
       "      <td id=\"T_dba34_row1_col9\" class=\"data row1 col9\" >0.504</td>\n",
       "      <td id=\"T_dba34_row1_col10\" class=\"data row1 col10\" >0.566</td>\n",
       "      <td id=\"T_dba34_row1_col11\" class=\"data row1 col11\" >0.558</td>\n",
       "      <td id=\"T_dba34_row1_col12\" class=\"data row1 col12\" >0.000</td>\n",
       "      <td id=\"T_dba34_row1_col13\" class=\"data row1 col13\" >0.504</td>\n",
       "      <td id=\"T_dba34_row1_col14\" class=\"data row1 col14\" >0.565</td>\n",
       "      <td id=\"T_dba34_row1_col15\" class=\"data row1 col15\" >0.558</td>\n",
       "      <td id=\"T_dba34_row1_col16\" class=\"data row1 col16\" >0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dba34_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_dba34_row2_col0\" class=\"data row2 col0\" >recce_fp32_trades</td>\n",
       "      <td id=\"T_dba34_row2_col1\" class=\"data row2 col1\" >0.917</td>\n",
       "      <td id=\"T_dba34_row2_col2\" class=\"data row2 col2\" >0.986</td>\n",
       "      <td id=\"T_dba34_row2_col3\" class=\"data row2 col3\" >0.986</td>\n",
       "      <td id=\"T_dba34_row2_col4\" class=\"data row2 col4\" >0.911</td>\n",
       "      <td id=\"T_dba34_row2_col5\" class=\"data row2 col5\" >0.310</td>\n",
       "      <td id=\"T_dba34_row2_col6\" class=\"data row2 col6\" >0.267</td>\n",
       "      <td id=\"T_dba34_row2_col7\" class=\"data row2 col7\" >0.359</td>\n",
       "      <td id=\"T_dba34_row2_col8\" class=\"data row2 col8\" >0.205</td>\n",
       "      <td id=\"T_dba34_row2_col9\" class=\"data row2 col9\" >0.917</td>\n",
       "      <td id=\"T_dba34_row2_col10\" class=\"data row2 col10\" >0.986</td>\n",
       "      <td id=\"T_dba34_row2_col11\" class=\"data row2 col11\" >0.986</td>\n",
       "      <td id=\"T_dba34_row2_col12\" class=\"data row2 col12\" >0.911</td>\n",
       "      <td id=\"T_dba34_row2_col13\" class=\"data row2 col13\" >0.916</td>\n",
       "      <td id=\"T_dba34_row2_col14\" class=\"data row2 col14\" >0.987</td>\n",
       "      <td id=\"T_dba34_row2_col15\" class=\"data row2 col15\" >0.986</td>\n",
       "      <td id=\"T_dba34_row2_col16\" class=\"data row2 col16\" >0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dba34_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_dba34_row3_col0\" class=\"data row3 col0\" >recce_int8dyn_standard</td>\n",
       "      <td id=\"T_dba34_row3_col1\" class=\"data row3 col1\" >0.901</td>\n",
       "      <td id=\"T_dba34_row3_col2\" class=\"data row3 col2\" >0.960</td>\n",
       "      <td id=\"T_dba34_row3_col3\" class=\"data row3 col3\" >0.969</td>\n",
       "      <td id=\"T_dba34_row3_col4\" class=\"data row3 col4\" >0.894</td>\n",
       "      <td id=\"T_dba34_row3_col5\" class=\"data row3 col5\" >nan</td>\n",
       "      <td id=\"T_dba34_row3_col6\" class=\"data row3 col6\" >nan</td>\n",
       "      <td id=\"T_dba34_row3_col7\" class=\"data row3 col7\" >nan</td>\n",
       "      <td id=\"T_dba34_row3_col8\" class=\"data row3 col8\" >nan</td>\n",
       "      <td id=\"T_dba34_row3_col9\" class=\"data row3 col9\" >0.900</td>\n",
       "      <td id=\"T_dba34_row3_col10\" class=\"data row3 col10\" >0.960</td>\n",
       "      <td id=\"T_dba34_row3_col11\" class=\"data row3 col11\" >0.969</td>\n",
       "      <td id=\"T_dba34_row3_col12\" class=\"data row3 col12\" >0.893</td>\n",
       "      <td id=\"T_dba34_row3_col13\" class=\"data row3 col13\" >0.895</td>\n",
       "      <td id=\"T_dba34_row3_col14\" class=\"data row3 col14\" >0.959</td>\n",
       "      <td id=\"T_dba34_row3_col15\" class=\"data row3 col15\" >0.968</td>\n",
       "      <td id=\"T_dba34_row3_col16\" class=\"data row3 col16\" >0.887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dba34_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_dba34_row4_col0\" class=\"data row4 col0\" >recce_int8dyn_pgd_at</td>\n",
       "      <td id=\"T_dba34_row4_col1\" class=\"data row4 col1\" >0.504</td>\n",
       "      <td id=\"T_dba34_row4_col2\" class=\"data row4 col2\" >0.554</td>\n",
       "      <td id=\"T_dba34_row4_col3\" class=\"data row4 col3\" >0.547</td>\n",
       "      <td id=\"T_dba34_row4_col4\" class=\"data row4 col4\" >0.000</td>\n",
       "      <td id=\"T_dba34_row4_col5\" class=\"data row4 col5\" >nan</td>\n",
       "      <td id=\"T_dba34_row4_col6\" class=\"data row4 col6\" >nan</td>\n",
       "      <td id=\"T_dba34_row4_col7\" class=\"data row4 col7\" >nan</td>\n",
       "      <td id=\"T_dba34_row4_col8\" class=\"data row4 col8\" >nan</td>\n",
       "      <td id=\"T_dba34_row4_col9\" class=\"data row4 col9\" >0.504</td>\n",
       "      <td id=\"T_dba34_row4_col10\" class=\"data row4 col10\" >0.553</td>\n",
       "      <td id=\"T_dba34_row4_col11\" class=\"data row4 col11\" >0.546</td>\n",
       "      <td id=\"T_dba34_row4_col12\" class=\"data row4 col12\" >0.000</td>\n",
       "      <td id=\"T_dba34_row4_col13\" class=\"data row4 col13\" >0.504</td>\n",
       "      <td id=\"T_dba34_row4_col14\" class=\"data row4 col14\" >0.554</td>\n",
       "      <td id=\"T_dba34_row4_col15\" class=\"data row4 col15\" >0.546</td>\n",
       "      <td id=\"T_dba34_row4_col16\" class=\"data row4 col16\" >0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dba34_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_dba34_row5_col0\" class=\"data row5 col0\" >recce_int8dyn_trades</td>\n",
       "      <td id=\"T_dba34_row5_col1\" class=\"data row5 col1\" >0.917</td>\n",
       "      <td id=\"T_dba34_row5_col2\" class=\"data row5 col2\" >0.986</td>\n",
       "      <td id=\"T_dba34_row5_col3\" class=\"data row5 col3\" >0.986</td>\n",
       "      <td id=\"T_dba34_row5_col4\" class=\"data row5 col4\" >0.911</td>\n",
       "      <td id=\"T_dba34_row5_col5\" class=\"data row5 col5\" >nan</td>\n",
       "      <td id=\"T_dba34_row5_col6\" class=\"data row5 col6\" >nan</td>\n",
       "      <td id=\"T_dba34_row5_col7\" class=\"data row5 col7\" >nan</td>\n",
       "      <td id=\"T_dba34_row5_col8\" class=\"data row5 col8\" >nan</td>\n",
       "      <td id=\"T_dba34_row5_col9\" class=\"data row5 col9\" >0.917</td>\n",
       "      <td id=\"T_dba34_row5_col10\" class=\"data row5 col10\" >0.986</td>\n",
       "      <td id=\"T_dba34_row5_col11\" class=\"data row5 col11\" >0.986</td>\n",
       "      <td id=\"T_dba34_row5_col12\" class=\"data row5 col12\" >0.911</td>\n",
       "      <td id=\"T_dba34_row5_col13\" class=\"data row5 col13\" >0.916</td>\n",
       "      <td id=\"T_dba34_row5_col14\" class=\"data row5 col14\" >0.986</td>\n",
       "      <td id=\"T_dba34_row5_col15\" class=\"data row5 col15\" >0.986</td>\n",
       "      <td id=\"T_dba34_row5_col16\" class=\"data row5 col16\" >0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dba34_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_dba34_row6_col0\" class=\"data row6 col0\" >recce_int8static_fx_standard</td>\n",
       "      <td id=\"T_dba34_row6_col1\" class=\"data row6 col1\" >0.899</td>\n",
       "      <td id=\"T_dba34_row6_col2\" class=\"data row6 col2\" >0.960</td>\n",
       "      <td id=\"T_dba34_row6_col3\" class=\"data row6 col3\" >0.968</td>\n",
       "      <td id=\"T_dba34_row6_col4\" class=\"data row6 col4\" >0.891</td>\n",
       "      <td id=\"T_dba34_row6_col5\" class=\"data row6 col5\" >nan</td>\n",
       "      <td id=\"T_dba34_row6_col6\" class=\"data row6 col6\" >nan</td>\n",
       "      <td id=\"T_dba34_row6_col7\" class=\"data row6 col7\" >nan</td>\n",
       "      <td id=\"T_dba34_row6_col8\" class=\"data row6 col8\" >nan</td>\n",
       "      <td id=\"T_dba34_row6_col9\" class=\"data row6 col9\" >0.897</td>\n",
       "      <td id=\"T_dba34_row6_col10\" class=\"data row6 col10\" >0.959</td>\n",
       "      <td id=\"T_dba34_row6_col11\" class=\"data row6 col11\" >0.968</td>\n",
       "      <td id=\"T_dba34_row6_col12\" class=\"data row6 col12\" >0.888</td>\n",
       "      <td id=\"T_dba34_row6_col13\" class=\"data row6 col13\" >0.891</td>\n",
       "      <td id=\"T_dba34_row6_col14\" class=\"data row6 col14\" >0.959</td>\n",
       "      <td id=\"T_dba34_row6_col15\" class=\"data row6 col15\" >0.967</td>\n",
       "      <td id=\"T_dba34_row6_col16\" class=\"data row6 col16\" >0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dba34_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_dba34_row7_col0\" class=\"data row7 col0\" >recce_int8qat_fx_standard</td>\n",
       "      <td id=\"T_dba34_row7_col1\" class=\"data row7 col1\" >0.888</td>\n",
       "      <td id=\"T_dba34_row7_col2\" class=\"data row7 col2\" >0.967</td>\n",
       "      <td id=\"T_dba34_row7_col3\" class=\"data row7 col3\" >0.974</td>\n",
       "      <td id=\"T_dba34_row7_col4\" class=\"data row7 col4\" >0.875</td>\n",
       "      <td id=\"T_dba34_row7_col5\" class=\"data row7 col5\" >nan</td>\n",
       "      <td id=\"T_dba34_row7_col6\" class=\"data row7 col6\" >nan</td>\n",
       "      <td id=\"T_dba34_row7_col7\" class=\"data row7 col7\" >nan</td>\n",
       "      <td id=\"T_dba34_row7_col8\" class=\"data row7 col8\" >nan</td>\n",
       "      <td id=\"T_dba34_row7_col9\" class=\"data row7 col9\" >0.887</td>\n",
       "      <td id=\"T_dba34_row7_col10\" class=\"data row7 col10\" >0.968</td>\n",
       "      <td id=\"T_dba34_row7_col11\" class=\"data row7 col11\" >0.974</td>\n",
       "      <td id=\"T_dba34_row7_col12\" class=\"data row7 col12\" >0.875</td>\n",
       "      <td id=\"T_dba34_row7_col13\" class=\"data row7 col13\" >0.878</td>\n",
       "      <td id=\"T_dba34_row7_col14\" class=\"data row7 col14\" >0.968</td>\n",
       "      <td id=\"T_dba34_row7_col15\" class=\"data row7 col15\" >0.974</td>\n",
       "      <td id=\"T_dba34_row7_col16\" class=\"data row7 col16\" >0.862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x289b796e270>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./runs\\recce_robust_summary.json | ./runs\\recce_robust_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os, copy, json, numpy as np, pandas as pd, torch\n",
    "from pathlib import Path\n",
    "\n",
    "need_syms = [\n",
    "    \"make_model\", \"train\", \"evaluate\", \"evaluate_under_pgd\",\n",
    "    \"quantize_dynamic\", \"device\", \"train_ds\", \"test_loader\",\n",
    "    \"PGD_EPS\", \"PGD_ALPHA\", \"PGD_STEPS\", \"TRADES_BETA\",\n",
    "    \"evaluate_int8_cpu\"  # from Part 8 helpers\n",
    "]\n",
    "missing = [n for n in need_syms if n not in globals()]\n",
    "assert not missing, f\"Missing symbols from earlier parts: {missing}\"\n",
    "\n",
    "RUN_DIR = \"./runs\"; os.makedirs(RUN_DIR, exist_ok=True)\n",
    "MODEL_TAG = \"recce\"\n",
    "\n",
    "\n",
    "DO_TRAIN_PGD_AT   = True    \n",
    "DO_TRAIN_TRADES   = True   \n",
    "DO_AUTOATTACK     = False   \n",
    "AUTOATTACK_EPS    = PGD_EPS \n",
    "JPEG_QUALITIES    = [90, 70]  \n",
    "\n",
    "\n",
    "ROBUST_EPOCHS = 8\n",
    "\n",
    "\n",
    "if DO_AUTOATTACK:\n",
    "    get_ipython().system(\"pip -q install autoattack\")\n",
    "    from autoattack import AutoAttack\n",
    "else:\n",
    "    AutoAttack = None\n",
    "\n",
    "def autoattack_acc(model, loader, eps=AUTOATTACK_EPS):\n",
    "    \"\"\"AutoAttack accuracy for FP32 models (GPU).\"\"\"\n",
    "    assert AutoAttack is not None, \"AutoAttack not enabled (DO_AUTOATTACK=False)\"\n",
    "    model = model.to(device).eval()\n",
    "    adversary = AutoAttack(model, norm='Linf', eps=eps, version='standard', device=device)\n",
    "    total = correct = 0\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        x_adv = adversary.run_standard_evaluation(x, y, bs=x.size(0))\n",
    "        with torch.no_grad():\n",
    "            pred = model(x_adv).argmax(1)\n",
    "        correct += (pred==y).sum().item()\n",
    "        total   += y.numel()\n",
    "    return correct/total\n",
    "\n",
    "from PIL import Image\n",
    "import io, torchvision.transforms.functional as TF\n",
    "mean = (0.485, 0.456, 0.406); std = (0.229, 0.224, 0.225)\n",
    "\n",
    "def jpeg_compress_tensor(x, quality=70):\n",
    "    x_cpu = x.detach().cpu().clamp(0,1)\n",
    "    out = []\n",
    "    for i in range(x_cpu.size(0)):\n",
    "        img = TF.to_pil_image(x_cpu[i])\n",
    "        buf = io.BytesIO(); img.save(buf, format=\"JPEG\", quality=quality); buf.seek(0)\n",
    "        out.append(TF.to_tensor(Image.open(buf)))\n",
    "    return torch.stack(out, dim=0).to(x.device)\n",
    "\n",
    "def eval_jpeg_fp32(model, loader, quality=70):\n",
    "    \"\"\"JPEG robustness for FP32 models (on GPU).\"\"\"\n",
    "    model = model.to(device).eval()\n",
    "    from torchmetrics.classification import BinaryAUROC, BinaryAveragePrecision, BinaryF1Score\n",
    "    auroc, auprc, f1 = BinaryAUROC().to(device), BinaryAveragePrecision().to(device), BinaryF1Score().to(device)\n",
    "    correct=total=0\n",
    "    mean_t = torch.tensor(mean, device=device).view(1,3,1,1)\n",
    "    std_t  = torch.tensor(std,  device=device).view(1,3,1,1)\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            x_den = x*std_t + mean_t\n",
    "            x_jpg = jpeg_compress_tensor(x_den, quality=quality)\n",
    "            x_nrm = (x_jpg - mean_t)/std_t\n",
    "            logits = model(x_nrm)\n",
    "            prob = torch.softmax(logits, dim=1)[:,1]\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred==y).sum().item(); total += y.numel()\n",
    "            auroc.update(prob, y); auprc.update(prob, y); f1.update(pred, y)\n",
    "    return {\"acc\":correct/total,\"auroc\":auroc.compute().item(),\"auprc\":auprc.compute().item(),\"f1\":f1.compute().item()}\n",
    "\n",
    "def eval_jpeg_int8(model_cpu, loader, quality=70):\n",
    "    \"\"\"\n",
    "    JPEG robustness for INT8 models (Dynamic/Static/QAT).\n",
    "    Everything stays on CPU: model and tensors.\n",
    "    \"\"\"\n",
    "    from torchmetrics.classification import BinaryAUROC, BinaryAveragePrecision, BinaryF1Score\n",
    "    m = copy.deepcopy(model_cpu).to(\"cpu\").eval()\n",
    "    auroc = BinaryAUROC().to(\"cpu\")\n",
    "    auprc = BinaryAveragePrecision().to(\"cpu\")\n",
    "    f1    = BinaryF1Score().to(\"cpu\")\n",
    "\n",
    "    mean_t = torch.tensor(mean).view(1,3,1,1)\n",
    "    std_t  = torch.tensor(std).view(1,3,1,1)\n",
    "\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            # keep on CPU\n",
    "            x_den = x*std_t + mean_t\n",
    "            x_jpg = jpeg_compress_tensor(x_den, quality=quality)\n",
    "            x_nrm = (x_jpg - mean_t)/std_t\n",
    "            logits = m(x_nrm)\n",
    "            prob = torch.softmax(logits, dim=1)[:,1]\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred==y).sum().item(); total += y.numel()\n",
    "            auroc.update(prob.cpu(), y.cpu())\n",
    "            auprc.update(prob.cpu(), y.cpu())\n",
    "            f1.update(pred.cpu(), y.cpu())\n",
    "    return {\n",
    "        \"acc\": float(correct/total),\n",
    "        \"auroc\": float(auroc.compute().item()),\n",
    "        \"auprc\": float(auprc.compute().item()),\n",
    "        \"f1\": float(f1.compute().item())\n",
    "    }\n",
    "\n",
    "records = []\n",
    "\n",
    "def eval_all_fp32(tag, m):\n",
    "    \"\"\"\n",
    "    Evaluate a FP32 model:\n",
    "      - Clean metrics\n",
    "      - PGD-10 metrics\n",
    "      - Optional AutoAttack\n",
    "      - JPEG robustness at JPEG_QUALITIES\n",
    "    \"\"\"\n",
    "    row = {\"ModelTag\": tag}\n",
    "    # Clean\n",
    "    met_clean = evaluate(m.to(device), test_loader)\n",
    "    row.update({f\"clean_{k}\": float(v) for k,v in met_clean.items()})\n",
    "    # PGD-10 (white-box)\n",
    "    met_pgd = evaluate_under_pgd(m, test_loader, steps=10)\n",
    "    row.update({f\"pgd_{k}\": float(v) for k,v in met_pgd.items()})\n",
    "    # AutoAttack (black-box)\n",
    "    if DO_AUTOATTACK:\n",
    "        aa_acc = autoattack_acc(m, test_loader, eps=AUTOATTACK_EPS)\n",
    "        row[\"aa_acc\"] = float(aa_acc)\n",
    "    # JPEG robustness\n",
    "    for q in JPEG_QUALITIES:\n",
    "        mj = eval_jpeg_fp32(m, test_loader, quality=q)\n",
    "        for k,v in mj.items():\n",
    "            row[f\"jpeg{q}_{k}\"] = float(v)\n",
    "    return row\n",
    "\n",
    "def eval_all_int8(tag, m_cpu):\n",
    "    \"\"\"\n",
    "    Evaluate an INT8 model (Dynamic/Static/QAT) on:\n",
    "      - Clean metrics (CPU, via evaluate_int8_cpu)\n",
    "      - JPEG robustness\n",
    "    We DO NOT run PGD or AutoAttack on quantized models (gradients & support are unreliable).\n",
    "    \"\"\"\n",
    "    row = {\"ModelTag\": tag}\n",
    "    met_clean = evaluate_int8_cpu(m_cpu, test_loader)\n",
    "    row.update({f\"clean_{k}\": float(v) for k,v in met_clean.items()})\n",
    "    for q in JPEG_QUALITIES:\n",
    "        mj = eval_jpeg_int8(m_cpu, test_loader, quality=q)\n",
    "        for k,v in mj.items():\n",
    "            row[f\"jpeg{q}_{k}\"] = float(v)\n",
    "    return row\n",
    "\n",
    "baseline_fp32 = copy.deepcopy(model).to(device).eval()\n",
    "records.append(eval_all_fp32(f\"{MODEL_TAG}_fp32_standard\", baseline_fp32))\n",
    "\n",
    "if DO_TRAIN_PGD_AT:\n",
    "    model_pgd = make_model(MODEL_TAG)\n",
    "    model_pgd = train(model_pgd, regime=\"pgd_at\", epochs=ROBUST_EPOCHS)\n",
    "    torch.save(model_pgd.state_dict(), os.path.join(RUN_DIR, f\"{MODEL_TAG}_pgdat_fp32.pt\"))\n",
    "    records.append(eval_all_fp32(f\"{MODEL_TAG}_fp32_pgd_at\", model_pgd.eval()))\n",
    "\n",
    "if DO_TRAIN_TRADES:\n",
    "    model_trd = make_model(MODEL_TAG)\n",
    "    model_trd = train(model_trd, regime=\"trades\", epochs=ROBUST_EPOCHS)\n",
    "    torch.save(model_trd.state_dict(), os.path.join(RUN_DIR, f\"{MODEL_TAG}_trades_fp32.pt\"))\n",
    "    records.append(eval_all_fp32(f\"{MODEL_TAG}_fp32_trades\", model_trd.eval()))\n",
    "\n",
    "if \"dyn\" in globals():\n",
    "    dyn_base = dyn\n",
    "else:\n",
    "    dyn_base = quantize_dynamic(copy.deepcopy(baseline_fp32).to('cpu').eval())\n",
    "records.append(eval_all_int8(f\"{MODEL_TAG}_int8dyn_standard\", dyn_base))\n",
    "\n",
    "if DO_TRAIN_PGD_AT:\n",
    "    dyn_pgd = quantize_dynamic(copy.deepcopy(model_pgd).to('cpu').eval())\n",
    "    records.append(eval_all_int8(f\"{MODEL_TAG}_int8dyn_pgd_at\", dyn_pgd))\n",
    "if DO_TRAIN_TRADES:\n",
    "    dyn_trd = quantize_dynamic(copy.deepcopy(model_trd).to('cpu').eval())\n",
    "    records.append(eval_all_int8(f\"{MODEL_TAG}_int8dyn_trades\", dyn_trd))\n",
    "\n",
    "if \"st\" in globals():\n",
    "    records.append(eval_all_int8(f\"{MODEL_TAG}_int8static_fx_standard\", st))\n",
    "\n",
    "if \"qatm\" in globals():\n",
    "    records.append(eval_all_int8(f\"{MODEL_TAG}_int8qat_fx_standard\", qatm))\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "front_cols = [\n",
    "    \"clean_acc\",\"clean_auroc\",\"clean_auprc\",\"clean_f1\",\n",
    "    \"pgd_acc\",\"pgd_auroc\",\"pgd_auprc\",\"pgd_f1\"\n",
    "]\n",
    "aa_col = [\"aa_acc\"] if DO_AUTOATTACK else []\n",
    "jpeg_cols = []\n",
    "for q in JPEG_QUALITIES:\n",
    "    jpeg_cols += [f\"jpeg{q}_acc\", f\"jpeg{q}_auroc\", f\"jpeg{q}_auprc\", f\"jpeg{q}_f1\"]\n",
    "\n",
    "ordered = [c for c in front_cols+aa_col+jpeg_cols if c in df.columns] + \\\n",
    "          [c for c in df.columns if c not in ([\"ModelTag\"]+front_cols+aa_col+jpeg_cols)]\n",
    "df = df[[\"ModelTag\"] + ordered]\n",
    "\n",
    "fmt = {c:\"{:.3f}\" for c in df.columns if c != \"ModelTag\"}\n",
    "display(df.style.format(fmt))\n",
    "\n",
    "json_path = os.path.join(RUN_DIR, f\"{MODEL_TAG}_robust_summary.json\")\n",
    "csv_path  = os.path.join(RUN_DIR, f\"{MODEL_TAG}_robust_summary.csv\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(df.to_dict(orient=\"records\"), f, indent=2)\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(\"Saved:\", json_path, \"|\", csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
